# -*- coding: utf-8 -*-
"""Self_sup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TNjxOZ_Fy2Ex33KmV9aqkgaTwn018FXf
"""

#Imports---------------------------------------------------------------
import torch
import torchvision
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image
#%matplotlib inline
#%config InlineBackend.figure_format = 'retina'

import glob
import numpy as np
from torch import optim
import torch.nn.functional as F
from torchvision import datasets, transforms, models
#Main pipeline for multiclass classification
#import libraries
import numpy as np
from time import time
import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from tqdm import tqdm
from sklearn.model_selection import KFold
from torchvision import models, transforms
import pickle
import os

from google.colab import drive
drive.mount('/content/drive')
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

#Parametros------------------------------------------------------------------------------------------------------------------------
K=5
arc='alexnet'
loss_func = nn.CrossEntropyLoss()
lr=0.00001
b_s=32
ep=50
PATH='/content/drive/My Drive/ML/model.pth'

#Standerdize the image----------------------------------------------------------------------
from torchvision.transforms import transforms
#from RandAugment import RandAugment
transform = transforms.Compose([            #[1]
 #transforms.Resize(224),                    #[2]
 #transforms.CenterCrop(224),                #[3]
 transforms.RandomHorizontalFlip(),
 transforms.RandomVerticalFlip(),
 transforms.ToTensor(),                     #[4]
 transforms.Normalize(                      #[5]
 mean=[0.485, 0.456, 0.406],                #[6]
 std=[0.229, 0.224, 0.225]                  #[7]
 )])
 

transform_strong = transforms.Compose([
 #transforms.Resize(224),                    #[2]
 #transforms.CenterCrop(224),                #[3]
 transforms.RandomHorizontalFlip(),
 transforms.RandomVerticalFlip(),
 torchvision.transforms.RandAugment(2,6),
 transforms.ToTensor(),                     #[4]
 transforms.Normalize(                      #[5]
 mean=[0.485, 0.456, 0.406],                #[6]
 std=[0.229, 0.224, 0.225]                  #[7]
 )])


#Abrir pickles com data------------------------------------------------------------------------------------------------------------------------
infile = open('/content/drive/My Drive/ML/NCI_data.pickle','rb')
data=pickle.load(infile)
infile.close
#print(data)
infile = open('/content/drive/My Drive/ML/data_u.pkl','rb')
data_u=pickle.load(infile)
infile.close
print(data_u[1])
i=0
x=[]
y=[]
while(i<len(data_u)):
 x.append(data_u[i][0])
 y.append(data_u[i][1])
 i=i+1


#Class Dataset------------------------------------------------------------------------------------------------------------------------
class MyDataset(Dataset):
   def __init__(self, type,datas, transform,strong, path,fold):
       if(type=='unlabeled_w' or type=='unlabeled_s'  ):
         
         self.X,self.Y=x,y
       else:  
         self.X,self.Y=datas[fold][type]
       self.transform = transform
       self.strong = strong
       self.path = path
       
   def __getitem__(self, i):
       PATH=self.path+self.X[i]
       while(True):
         try:
           X = Image.open(PATH)
           break
         except IOError:
           print('erro'+self.X[i])
       if(type=='unlabeled_w'):
         X = self.transform(X)
       elif(type=='unlabeled_s'):
         X = self.strong(X)  
       else:
         X = self.transform(X)
       Y = self.Y[i]
       #if(Y == -2.0):
        # Y=self.strong(X)
         #Y=10
       return X, Y

   def __len__(self):
       return len(self.X)

#Criar Dataloaders------------------------------------------------------------------------------------------------------------------------
ds= MyDataset('train',data,transform,transform_strong,'/content/drive/My Drive/ML/train/Transform/',1)
tr = DataLoader(ds, b_s, True)
#v_ds= MyDataset('val',transform,'/content/drive/My Drive/ML/CVT_1/Transform',1)
#val = DataLoader(v_ds, b_s, True)
t_ds= MyDataset('test',data,transform,transform_strong,'/content/drive/My Drive/ML/train/Transform/',1)
test = DataLoader(t_ds, b_s, True)
uw_ds= MyDataset('unlabeled_w',data_u,transform,transform_strong,'/content/drive/My Drive/ML/train/Transform/',1)
unl_w = DataLoader(uw_ds, b_s, True)
us_ds= MyDataset('unlabeled_s',data_u,transform,transform_strong,'/content/drive/My Drive/ML/train/Transform/',1)
unl_s = DataLoader(us_ds, b_s, True)

# Models------------------------------------------------------------------------------------------------------------------------
class Base(nn.Module):
    def __init__(self, pretrained_model, n_outputs):
        super().__init__()
        self.n_outputs = n_outputs
        model = getattr(models, pretrained_model)(pretrained=True) #using transfer learning from a pretrained model - True
        model = nn.Sequential(*tuple(model.children())[:-1])
        last_dimension = torch.flatten(model(torch.randn(1, 3, 224, 224))).shape[0]
        self.model = nn.Sequential(
            model,
            nn.Flatten(),
            nn.Dropout(0.4),
            nn.Linear(last_dimension, 512),
            nn.Dropout(0.4),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, K)
        )

    def forward(self, x):
        return self.model(x)

    def loss(self, Yhat, Y):
        return loss_func(Yhat, Y)

    def to_proba(self, Yhat):
        return F.softmax(Yhat, 1)

    def to_classes(self, Phat):
        return Phat.argmax(1)

    
    
#Criar a CNN------------------------------------------------------------------------------------------------------------------------    
#Give to the model the architecture and number of classes (K)
model = Base(arc, K)
#put the model on 'GPU' or 'CPU'
model = model.to(device)
#Define optimizer and scheduler 
optimizer = optim.Adam(model.parameters(),lr )

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)

#Treino------------------------------------------------------------------------------------------------------------------------
import torch.nn.functional as F
gama=0.2
thresh=0.8
running_loss=1010.0
train_acc=0.0
final_t_a=0.0
l_train=546*32
l_test=182*32
avg_acc=0.0
best_loss=1000.0
best_acc=0.0
torch.cuda.empty_cache()
best_acc_t=0
best_loss_t=0
PATH2='/content/drive/My Drive/ML/files/unl.pth'
labeled_iter = iter(tr)
unlabeledw_iter = iter(unl_w)
unlabeleds_iter = iter(unl_s)



for epoch in range(ep):
    print('\n')
    print('Iteração:',epoch)
    print('loss:',running_loss)
    print('acc:',avg_acc)
    torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': running_loss
            
            }, PATH2)
    end=0
    count=0
    while (end==0):
      count=count+1
      if(count==546):
        end=1
     
      
      inputs, labels = labeled_iter.next()
      weak,_ = unlabeledw_iter.next()
      strong,_ = unlabeleds_iter.next()


      inputs,labels = inputs.to(device),labels.to(device)
      weak,strong = weak.to(device),strong.to(device)

      #labeled data---------------------------------------------
      outputs = model(inputs)  
      Lx = F.cross_entropy(outputs, labels, reduction='mean')    
      loss_l = loss_func(outputs, labels)
      _, predicted = torch.max(outputs, 1)
      real=labels
      correct = (predicted == real).float().sum()
      avg_acc +=correct*100/(len(inputs)*32)

           
      #Unlabeled data----------------------------------------------------
      outputs = model(weak)
      outputs_s = model(strong)
      pseudo_label =  torch.softmax(outputs,dim=-1)
      pseudo_label,index = torch.max(pseudo_label,dim=-1)
      mask = pseudo_label.ge(thresh).float()
      Lu = (F.cross_entropy(outputs_s, index,reduction='none') * mask).mean()
      #loss_u = loss_func(outputs, index)*mask
     
      #Calculate both losses------------------------------------------------
      loss = Lx + gama*Lu
      #loss = loss_l + gama*loss_u
      loss.backward()
      optimizer.step()     
      running_loss += loss.item()
      print('.',end='')

      if avg_acc>=best_acc and running_loss<=best_loss :
             best_acc=avg_acc
             best_loss=running_loss
