{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h6ykJMSDW9by"
      },
      "outputs": [],
      "source": [
        "#Imports---------------------------------------------------------------\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "#%matplotlib inline\n",
        "#%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "#Main pipeline for multiclass classification\n",
        "#import libraries\n",
        "import numpy as np\n",
        "from time import time\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from torchvision import models, transforms\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "cZE3raABXCJG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "K=10\n",
        "arc='wideresnet'\n",
        "#loss_func = nn.CrossEntropyLoss()\n",
        "#loss_func =F.cross_entropy()\n",
        "\n",
        "lr=0.03\n",
        "b_s=64\n",
        "ep=50\n",
        "PATH='/content/drive/My Drive/ML/model.pth'\n",
        "PATH2='/content/drive/My Drive/ML/files/unl.pth'\n",
        "\n",
        "#APAGAR-------------------------------------\n",
        "#with open('/content/drive/My Drive/ML/Resultados/readme.txt', 'w') as f:\n",
        "#    f.write('')\n",
        "#with open('/content/drive/My Drive/ML/Resultados/readme.txt', 'a') as f:\n",
        "#    f.write('------------------------------------------------Self-Supervised\\nParametros:\\n\\nEpochs'+str(ep)+'\\nArc:'+arc+'\\nLearning rate:'+str(lr)+'\\nBatch size:'+str(b_s))"
      ],
      "metadata": {
        "id": "35n-xYJyXDoh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Datasets como v√£o buscar no Fix-match Pytorch-------------------------------------------------------------------------------\n",
        "##----------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "#from .randaugment import RandAugmentMC\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
        "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
        "normal_mean = (0.5, 0.5, 0.5)\n",
        "normal_std = (0.5, 0.5, 0.5)\n",
        "\n",
        "\n",
        "def get_cifar10(num_labeled,num_classes, root):\n",
        "    transform_labeled = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32,\n",
        "                              padding=int(32*0.125),\n",
        "                              padding_mode='reflect'),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "    ])\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "    ])\n",
        "    base_dataset = datasets.CIFAR10(root, train=True, download=True)\n",
        "\n",
        "    train_labeled_idxs, train_unlabeled_idxs = x_u_split(\n",
        "        num_labeled,num_classes, base_dataset.targets)\n",
        "\n",
        "    train_labeled_dataset = CIFAR10SSL(\n",
        "        root, train_labeled_idxs, train=True,\n",
        "        transform=transform_labeled)\n",
        "\n",
        "    train_unlabeled_dataset = CIFAR10SSL(\n",
        "        root, train_unlabeled_idxs, train=True,\n",
        "        transform=TransformFixMatch(mean=cifar10_mean, std=cifar10_std))\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(\n",
        "        root, train=False, transform=transform_val, download=False)\n",
        "\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, test_dataset\n",
        "\n",
        "\n",
        "def get_cifar100(args, root):\n",
        "\n",
        "    transform_labeled = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(size=32,\n",
        "                              padding=int(32*0.125),\n",
        "                              padding_mode='reflect'),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=cifar100_mean, std=cifar100_std)])\n",
        "\n",
        "    base_dataset = datasets.CIFAR100(\n",
        "        root, train=True, download=True)\n",
        "\n",
        "    train_labeled_idxs, train_unlabeled_idxs = x_u_split(\n",
        "        args, base_dataset.targets)\n",
        "\n",
        "    train_labeled_dataset = CIFAR100SSL(\n",
        "        root, train_labeled_idxs, train=True,\n",
        "        transform=transform_labeled)\n",
        "\n",
        "    train_unlabeled_dataset = CIFAR100SSL(\n",
        "        root, train_unlabeled_idxs, train=True,\n",
        "        transform=TransformFixMatch(mean=cifar100_mean, std=cifar100_std))\n",
        "\n",
        "    test_dataset = datasets.CIFAR100(\n",
        "        root, train=False, transform=transform_val, download=False)\n",
        "\n",
        "    return train_labeled_dataset, train_unlabeled_dataset, test_dataset\n",
        "\n",
        "\n",
        "def x_u_split(num_labeled,num_classes, labels):\n",
        "    label_per_class = num_labeled // num_classes\n",
        "    labels = np.array(labels)\n",
        "    labeled_idx = []\n",
        "    # unlabeled data: all data (https://github.com/kekmodel/FixMatch-pytorch/issues/10)\n",
        "    unlabeled_idx = np.array(range(len(labels)))\n",
        "    for i in range(num_classes):\n",
        "        idx = np.where(labels == i)[0]\n",
        "        idx = np.random.choice(idx, label_per_class, False)\n",
        "        labeled_idx.extend(idx)\n",
        "    labeled_idx = np.array(labeled_idx)\n",
        "    assert len(labeled_idx) == num_labeled\n",
        "\n",
        "   \n",
        "    np.random.shuffle(labeled_idx)\n",
        "    return labeled_idx, unlabeled_idx\n",
        "\n",
        "\n",
        "\n",
        "class TransformFixMatch(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.weak = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(size=32,\n",
        "                                  padding=int(32*0.125),\n",
        "                                  padding_mode='reflect')])\n",
        "        self.strong = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(size=32,\n",
        "                                  padding=int(32*0.125),\n",
        "                                  padding_mode='reflect'),\n",
        "            torchvision.transforms.RandAugment(2,10)])\n",
        "        self.normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        weak = self.weak(x)\n",
        "        strong = self.strong(x)\n",
        "        return self.normalize(weak), self.normalize(strong)\n",
        "\n",
        "\n",
        "class CIFAR10SSL(datasets.CIFAR10):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class CIFAR100SSL(datasets.CIFAR100):\n",
        "    def __init__(self, root, indexs, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super().__init__(root, train=train,\n",
        "                         transform=transform,\n",
        "                         target_transform=target_transform,\n",
        "                         download=download)\n",
        "        if indexs is not None:\n",
        "            self.data = self.data[indexs]\n",
        "            self.targets = np.array(self.targets)[indexs]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "DATASET_GETTERS = {'cifar10': get_cifar10,\n",
        "                   'cifar100': get_cifar100}"
      ],
      "metadata": {
        "id": "92VMpJBsyYwC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Cria datasets-------------------------------------------------------------------------------------------------------------  \n",
        "num_labeled=250\n",
        "num_classes=10\n",
        "\n",
        "labeled_dataset, unlabeled_dataset, test_dataset = get_cifar10(num_labeled,num_classes,'./data')\n",
        "\n",
        "labeled_dataset, unlabeled_dataset, test_dataset = DATASET_GETTERS['cifar10'](num_labeled,num_classes, './data')\n",
        "print(len(labeled_dataset))\n",
        "print(len(unlabeled_dataset))\n",
        "print(len(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGJkIFnzy9Gj",
        "outputId": "1cef0f07-daa3-47e4-b8d1-13163d2c3d86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "250\n",
            "50000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cria Dataloaders----------------------------------------------------------------------------------------------\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_sampler = RandomSampler #if args.local_rank == -1 else DistributedSampler\n",
        "labeled_trainloader = DataLoader(\n",
        "        labeled_dataset,\n",
        "        sampler=train_sampler(labeled_dataset),\n",
        "        batch_size=32,\n",
        "        #num_workers=4,\n",
        "        drop_last=True)\n",
        "\n",
        "unlabeled_trainloader = DataLoader(\n",
        "        unlabeled_dataset,\n",
        "        sampler=train_sampler(unlabeled_dataset),\n",
        "        batch_size=32*7,\n",
        "        #num_workers=4,\n",
        "        drop_last=True)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=SequentialSampler(test_dataset),\n",
        "        batch_size=32)\n",
        "        #num_workers=4)\n",
        "\n",
        "\n",
        "print(len(labeled_trainloader))\n",
        "print(len(unlabeled_trainloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1CGKPWj3iaQ",
        "outputId": "1aac00b4-329e-4fa3-d711-7fc203c298d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Dataset CIFAR 10 Feito por mim-------------------------------------------------------------------------------\n",
        "##----------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform_strong = transforms.Compose([\n",
        " #transforms.Resize(224),                    #[2]\n",
        " #transforms.CenterCrop(224),                #[3]\n",
        " transforms.RandomHorizontalFlip(),\n",
        " transforms.RandomVerticalFlip(),\n",
        " torchvision.transforms.RandAugment(2,5),\n",
        " transforms.ToTensor(),                     #[4]\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "counter=[0]*10\n",
        "labset=[]\n",
        "unlabset=[]\n",
        "for i in range (0,50000):\n",
        "  for z in range(0,10):\n",
        "    if(trainset[i][1]==z):\n",
        "      if(counter[z]<25):\n",
        "        counter[z]=counter[z]+1\n",
        "        labset.append(trainset[i])\n",
        "      else:\n",
        "        unlabset.append(trainset[i])  \n",
        "print(counter)\n",
        "print(len(labset))\n",
        "print(len(unlabset))  \n",
        "labloader = torch.utils.data.DataLoader(labset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_strong)\n",
        "counter=[0]*10\n",
        "labset2=[]\n",
        "unlabset=[]\n",
        "for i in range (0,50000):\n",
        "  for z in range(0,10):\n",
        "    if(trainset[i][1]==z):\n",
        "      if(counter[z]<25):\n",
        "        counter[z]=counter[z]+1\n",
        "        labset2.append(trainset[i])\n",
        "      else:\n",
        "        unlabset.append(trainset[i])  \n",
        "print(counter)\n",
        "print(len(labset))\n",
        "print(len(unlabset))  \n",
        "unlabloader = torch.utils.data.DataLoader(unlabset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "CgrK6nfmXFg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Dataseat Transform Unlabeled weak--------------------------------------------\n",
        "transform_simple = transforms.Compose([            #[1]\n",
        " #transforms.Resize(224),                    #[2]\n",
        " #transforms.CenterCrop(224),                #[3]\n",
        " transforms.RandomHorizontalFlip(),\n",
        " transforms.RandomVerticalFlip(),\n",
        " transforms.ToTensor(),                     #[4]\n",
        " transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_simple)\n",
        "counter=[0]*10\n",
        "labset3=[]\n",
        "unlabset_w=[]\n",
        "for i in range (0,50000):\n",
        "  for z in range(0,10):\n",
        "    if(trainset[i][1]==z):\n",
        "      if(counter[z]<25):\n",
        "        counter[z]=counter[z]+1\n",
        "        labset.append(trainset[i])\n",
        "      else:\n",
        "        unlabset_w.append(trainset[i])  \n",
        "\n",
        "labloader_w = torch.utils.data.DataLoader(unlabset_w, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "wI4HxmTEXF9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Vers√£o com mu=7----------------------------------------------------------\n",
        "labloader_w = torch.utils.data.DataLoader(unlabset_w, batch_size=batch_size*7,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "unlabloader = torch.utils.data.DataLoader(unlabset, batch_size=batch_size*7,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "7LQ-NiHAeelk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## AlexNet usada no modelo normal---------------------------------------------------\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=K):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256 * 2 * 2, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, K),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 2 * 2)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uGdj6703XHWj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Wideresnet usada por eles-------------------------------------------------------------------------------\n",
        "##----------------------------------------------------------------------------------------------------------------------------\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def mish(x):\n",
        "    \"\"\"Mish: A Self Regularized Non-Monotonic Neural Activation Function (https://arxiv.org/abs/1908.08681)\"\"\"\n",
        "    return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "\n",
        "class PSBatchNorm2d(nn.BatchNorm2d):\n",
        "    \"\"\"How Does BN Increase Collapsed Neural Network Filters? (https://arxiv.org/abs/2001.11216)\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, alpha=0.1, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True):\n",
        "        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x) + self.alpha\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0, activate_before_residual=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=0.001)\n",
        "        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes, momentum=0.001)\n",
        "        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.drop_rate = drop_rate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                                                padding=0, bias=False) or None\n",
        "        self.activate_before_residual = activate_before_residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut and self.activate_before_residual == True:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.drop_rate > 0:\n",
        "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate=0.0, activate_before_residual=False):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(\n",
        "            block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual)\n",
        "\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes,\n",
        "                                i == 0 and stride or 1, drop_rate, activate_before_residual))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, num_classes, depth=28, widen_factor=2, drop_rate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        channels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(\n",
        "            n, channels[0], channels[1], block, 1, drop_rate, activate_before_residual=True)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(\n",
        "            n, channels[1], channels[2], block, 2, drop_rate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(\n",
        "            n, channels[2], channels[3], block, 2, drop_rate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(channels[3], momentum=0.001)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "        self.channels = channels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,\n",
        "                                        mode='fan_out',\n",
        "                                        nonlinearity='leaky_relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1.0)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = out.view(-1, self.channels)\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "XzHekfRsjsPZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Give to the model the architecture and number of classes (K)\n",
        "model = WideResNet(K)\n",
        "#put the model on 'GPU' or 'CPU'\n",
        "model = model.to(device)\n",
        "#Define optimizer and scheduler \n",
        "optimizer = optim.Adam(model.parameters(),lr )\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)"
      ],
      "metadata": {
        "id": "BH5L7OcMXOL3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def interleave(x, size):\n",
        "    s = list(x.shape)\n",
        "    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
        "\n",
        "\n",
        "def de_interleave(x, size):\n",
        "    s = list(x.shape)\n",
        "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
        "\n",
        "gama=0\n",
        "thresh=0.95\n",
        "running_loss=1010.0\n",
        "train_acc=0.0\n",
        "final_t_a=0.0\n",
        "l_train=547*32\n",
        "l_test=182*32\n",
        "avg_acc=0.0\n",
        "best_loss=1000.0\n",
        "best_acc=0.0\n",
        "torch.cuda.empty_cache()\n",
        "best_acc_t=0\n",
        "best_loss_t=0\n",
        "PATH2='/content/drive/My Drive/ML/files/unl.pth'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    print('\\n')\n",
        "    print('Itera√ß√£o:',epoch)\n",
        "    print('loss:',running_loss)\n",
        "    print('acc:',avg_acc)\n",
        "  #  torch.save({\n",
        "  #           'epoch': epoch,\n",
        "  #           'model_state_dict': model.state_dict(),\n",
        "  #          'optimizer_state_dict': optimizer.state_dict(),\n",
        "  #          'loss': running_loss\n",
        "  #          \n",
        "  #          }, PATH2)\n",
        "    end=0\n",
        "    count=1\n",
        "    #unlabeledw_iter = iter(labloader_w)\n",
        "    unlabeled_iter = iter(unlabeled_trainloader)\n",
        "    labeled_iter = iter(labeled_trainloader)\n",
        "\n",
        "    if(epoch<=15):     \n",
        "      labeled_iter = iter(labeled_trainloader)\n",
        "    avg_acc=0.0\n",
        "    running_loss=0.0\n",
        "    waiting=0\n",
        "    while (end==0):\n",
        "     if(epoch>15): \n",
        "      count=count+1\n",
        "      \n",
        "   \n",
        "      try:\n",
        "       inputs, labels = labeled_iter.next()\n",
        "      except StopIteration:\n",
        "         end=1\n",
        "         \n",
        "      try:  \n",
        "        (weak,strong),_ = unlabeled_iter.next()\n",
        "        #weak,_ = unlabeledw_iter.next()\n",
        "        \n",
        "        \n",
        "      except StopIteration:\n",
        "        end=1  \n",
        "        \n",
        "      if(end==0):  \n",
        "        inputs,labels = inputs.to(device),labels.to(device)\n",
        "        weak,strong = weak.to(device),strong.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        batch_size = inputs.shape[0]\n",
        "        #labeled data---------------------------------------------\n",
        "        inp = interleave(\n",
        "                torch.cat((inputs, weak, strong)), 2*7+1).to(device)\n",
        "        print(len(inp))\n",
        "        logits = model(inp)\n",
        "        logits = de_interleave(logits, 2*7+1)\n",
        "        logits_x = logits[:batch_size]\n",
        "        logits_u_w, logits_u_s = logits[batch_size:].chunk(2) \n",
        "        del logits\n",
        "        \n",
        "        #outputs=model(inputs)\n",
        "        Lx = F.cross_entropy(logits_x, labels, reduction='mean')    \n",
        "        #loss_l = loss_func(outputs, labels)\n",
        "        _, predicted = torch.max(logits_x, 1)\n",
        "        real=labels\n",
        "        correct = (predicted == real).float().sum()\n",
        "        avg_acc +=correct*100/(32*len(unlabeled_iter))\n",
        "\n",
        "\n",
        "           \n",
        "        #Unlabeled data----------------------------------------------------\n",
        "        #outputs = model(weak)\n",
        "        #outputs_s = model(strong)\n",
        "        pseudo_label =  torch.softmax(logits_u_s,dim=-1)\n",
        "        pseudo_label,index = torch.max(pseudo_label,dim=-1)\n",
        "        mask = pseudo_label.ge(thresh).float()\n",
        "        Lu = (F.cross_entropy(logits_u_s, index,reduction='none') * mask).mean()\n",
        "        #loss_u = loss_func(outputs, index)*mask\n",
        "     \n",
        "        #Calculate both losses------------------------------------------------\n",
        "        loss = Lx + gama*Lu\n",
        "        #loss = loss_l + gama*loss_u\n",
        "        loss.backward()\n",
        "        optimizer.step()     \n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        print('.',end='')\n",
        "\n",
        "        if avg_acc>=best_acc and running_loss<=best_loss :\n",
        "             best_acc=avg_acc\n",
        "             best_loss=running_loss\n",
        "     else:\n",
        "        count=count+1\n",
        "        if(count==112):\n",
        "          end=1\n",
        "     \n",
        "        try:\n",
        "          inputs, labels = labeled_iter.next()\n",
        "          #weak,_ = unlabeledw_iter.next()\n",
        "          #strong,_ = unlabeleds_iter.next()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "        except StopIteration:\n",
        "          end=1\n",
        "        if(end==0):  \n",
        "          inputs,labels = inputs.to(device),labels.to(device)\n",
        "          #weak,strong = weak.to(device),strong.to(device)\n",
        "\n",
        "          #labeled data---------------------------------------------\n",
        "          outputs = model(inputs)  \n",
        "          Lx = F.cross_entropy(outputs, labels, reduction='mean')    \n",
        "          #loss_l = loss_func(outputs, labels)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          real=labels\n",
        "          correct = (predicted == real).float().sum()\n",
        "          avg_acc +=correct*100/(32*len(labeled_iter))\n",
        "\n",
        "      \n",
        "          loss = Lx #+ gama*Lu\n",
        "          #loss = loss_l + gama*loss_u\n",
        "          loss.backward()\n",
        "          optimizer.step()     \n",
        "          \n",
        "          running_loss += loss.item()\n",
        "          print('.',end='')\n",
        "\n",
        "          if avg_acc>=best_acc and running_loss<=best_loss :\n",
        "             best_acc=avg_acc\n",
        "             best_loss=running_loss    \n",
        "    if(epoch>20):\n",
        "      scheduler.step(running_loss)             "
      ],
      "metadata": {
        "id": "PJD6Vg8SXQDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "running_loss=0.0\n",
        "train_acc=0.0\n",
        "final_t_a=0.0\n",
        "l= len(test_loader)*32\n",
        "avg_acc=0.0\n",
        "\n",
        "\n",
        "##Test the dataset\n",
        "model.eval()\n",
        "for inputs,labels in test_loader:\n",
        "          inputs,labels = inputs.to(device),labels.to(device)\n",
        "          #weak,strong = weak.to(device),strong.to(device)\n",
        "\n",
        "          #labeled data---------------------------------------------\n",
        "          outputs = model(inputs)  \n",
        "          Lx = F.cross_entropy(outputs, labels, reduction='mean')    \n",
        "          #loss_l = loss_func(outputs, labels)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          real=labels\n",
        "          correct = (predicted == real).float().sum()\n",
        "          avg_acc +=correct*100/l\n",
        "      \n",
        "          loss = Lx #+ gama*Lu\n",
        "          #loss = loss_l + gama*loss_u\n",
        "          #loss.backward()\n",
        "          #optimizer.step()     \n",
        "          running_loss += loss.item() \n",
        "          print('.',end='')\n",
        "\n",
        "print('\\n')\n",
        "print('loss:',running_loss)\n",
        "print('acc:',avg_acc)\n",
        "print('Finished Validation')"
      ],
      "metadata": {
        "id": "UsxKECtYeE0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs,labels in labloader:\n",
        "  print(labels)"
      ],
      "metadata": {
        "id": "YX3BSkDvhkjR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}